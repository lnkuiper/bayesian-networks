# Structure learning
In this section we automatically construct a Bayesian network structure for our
dataset using a structure learning algorithm.  The PC algorithm (named after its
authors, Peter and Clark) is the state-of-the-art constraint based method for
causal discovery.  It is based on the Inductive Causation (IC) algorithm by
Pearl and Verma, 1991.  It uses many conditional independence tests, similar to
the tests we performed in the previous section.

## Software
The `bnlearn` package implements various structure learning algorithms.  Their
implementation of PC is called `pc.stable`, which takes the data, and many
parameters, of which at least three must be considered before learning network
structure, which are the following.

\medskip
\begin{tabular}{ll}
    \textbf{variable}   & \textbf{type} \\
    \texttt{test}       & the conditional independence test to \\
                        & use \\
    \texttt{alpha}      & target error rate \\
    \texttt{max.sx}     & maximum size of conditioning sets \\
                        & used in independence tests \\
\end{tabular}
\medskip

## Early observations
In the previous section we binned the data to nominal values to use the $\chi^2$
test for conditional independencies.  An RMSEA value greater than 0.05 was
considered to be too high for these tests, and a maximum size of the
conditioning sets was not set.  We aim to learn a network structure from the
data that we can compare to the model that we created ourselves, so our first
structure learning approach was to use `pc.stable` with `test = 'x2', alpha =
0.05, max.sx = NULL`.  This gave us an extremely sparse graph with many
variables not having an edge at all.  Changing the test to other tests that are
suitable for nominal values, as well as changing the other two parameters had
little to no effect on the outcome.  We assume the main reason for this to be
that many combinations of variables have too few samples, which causes the
conditional independence tests for nominal variables to be unreliable or even
undefined, especially when the size of the conditioning set increases.  This
leads us to believe that we cannot work with nominal values if we wish to learn
a network structure that can be compared with the network that we created in the
previous section.

## Approach
We aim to use the correlation test for independence, because it is considered to
be more robust, and mainly, it does not suffer from the problem that we
encountered.  This test needs continuous variables.  To convert our data to
continuous, we must again look into the data, as well as use our domain
knowledge to convert all variables to  `numeric` type in `R`.

### Pre-processing
We start again from the unbinned data, with 41188 data points.  The same
variables that were dropped in the the previous section are dropped from the
dataset again.  That means that the variables _contact, month, day_of_week,
duration, compaign, pdays, previous, poutcome_ and _euribor3m_ are out.

We will now discuss how the remaining variables were transformed to be
continuous.

#### Age
This variable was already an integer, so we simply cast it to numeric.

#### Job
This variable has 12 levels: _admin., blue-collar, entrepreneur, housemaid,
management, retired, self-employed, services, student, technician, unemployed,
unknown_.  The levels were grouped based on salary, then mapped to a numeric
value.  The dataset is from Portugal, but data from
[usnews.com](https://money.usnews.com/) was used to estimate income, because it
is publicly available.  We came up with the following groups along with their
values:

  1. _unknown, unemployed, retired, student_
  2. _housemaid, services, blue-coller_
  3. _admin., technician_
  4. _self-employed, entrepreneur_
  5. _management_

#### Marital
This variable has 4 levels: _divorced, married, single, unknown_.
There were only 80 _unknown_ entries, so these were added to _single_.
Then, from this variable, the three binary variables _divorced, married, single_
were created. These are often called 'dummy variables'.

#### Education
This variable has 8 levels, which were simply ordered 1 through 9 from low to
high, before being cast to numeric. The order chosen is: _unknown, illiterate,
basic.4y, basic.6y, basic.9y, high.school, professional.course, 
university.degree_.

#### Default
This variable has 3 levels: _no, unknown, yes_.
There were only 3 _yes_ entries, so these were added to _unknown_.
The variable was then made binary with 0 representing _no_, and 1 representing _unknown_.

#### Housing & Loan
These variables both have 3 levels: _no, unknown, yes_.
There were exactly 990 _unknown_ entries in both, these were added to _no_.
The variable was then made binary with 0 representing _no_, and 1 representing _yes_.

#### Economic variables
The four economic variables _emp.var.rate, cons.price.idx, cons.conf.idx, nr.employed_ are already numeric, and were not altered.

#### Outcome
Outcome variable _y_ has 2 levels: _no, yes_.
These were simply mapped to 0 and 1, respectively.

### Automatic network construction
Now that the variables are converted to `numeric`, `pc.stable` with correlation
as independence test can be applied to the data, and the network structure can
be automatically learned. Without tweaking the parameters, e.g. `test = 'cor',
alpha = 0.05, max.sx = NULL` we get a DAG that has many edges, and resembles 
the final DAG from testing much more already.

However, there are some problems. Since the network is ignorant of the order in
which variables happen, it learns that the outcome variable, _y_, is the
predictor of some other variables. We do not want this to happen, therefore we
add some edges to the `blacklist` parameter of `pc.stable`. Namely all outgoing
edges from _y_, as well as all incoming edges to `age`, since age should not be
influenced by other variables.

The learned network structure was inspected for many values of `alpha, max.sx`.
We found that resulting DAGs with the maximum amount of conditioning variables
set to 3 or greater did not differ. When it was set to lower than 3, many edges
were introduced, resulting in an almost fully connected graph. We consider 3
conditioning variables to still be on the low side. Therefore we chose to set
it to `NULL`, an unbounded amount of conditioning variables, because it did not
change when greater than 3.

Tweaking the `alpha` parameter between 0.05 and 0.3 did not change the learned
network. Setting it outside of this range either resulted in a network with too
many edges, or with too few edges. We settled on `alpha = 0.1`. 

## Learned DAG
The DAG that was learned with our final parameter configuration is shown in 
\autoref{fig:learned_dag_small}. A bigger version of this DAG is shown in the
appendix (\autoref{sec:learned_dag} \autoref{fig:learned_dag}). 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{images/learned_dag}
  \caption{Learned DAG with PC algorithm.}
  \label{fig:learned_dag_small}
\end{figure}

Many of the edges in the learned DAG are the same as in the DAG made by 
testing, save for some edges, and in different directions. We are happy to see
an edge from _education_ to _job_, because it makes a lot of sense. However, we
are surprised that neither of these two variables influence the outcome 
variable directly. In our final DAG these went into a latent variable _wealth_,
which directly influence the outcome.

We also expected that _housing_ and _loan_ would have an edge to _y_, but the
effect happens to be too small for the PC algorithm with our parameters. What 
is very interesting is that all four of the national economic indicator
variables are connected to _y_. In our final DAG only two were connected to the
outcome directly.

Finally, the algorithm finds a dependency between marital status and an 
economic variable, which we also found during model testing. Intuitively there
should not be a dependency there, but it could be there for a variety of
reasons.
